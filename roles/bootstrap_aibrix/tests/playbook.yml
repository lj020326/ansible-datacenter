---
# Example playbook usage (playbook.yml)
- name: Deploy AIBrix on Kubernetes
  hosts: localhost
  connection: local
  gather_facts: true
  vars:
    bootstrap_aibrix__version: "v0.3.0"
    bootstrap_aibrix__gpu_enabled: true
    bootstrap_aibrix__gpu_type: "a100"
    bootstrap_aibrix__gpu_count: 2
    bootstrap_aibrix__autoscaling_enabled: true
    bootstrap_aibrix__autoscaling_max_replicas: 20
    bootstrap_aibrix__monitoring_enabled: true
    bootstrap_aibrix__ingress_enabled: true
    bootstrap_aibrix__domain: "llm.mycompany.com"
  roles:
    - bootstrap_aibrix

  post_tasks:
    - name: Display AIBrix access information
      ansible.builtin.debug:
        msg: |
          AIBrix has been successfully deployed!
          
          Access Information:
          {% if bootstrap_aibrix__ingress_enabled %}
          - External URL: https://{{ bootstrap_aibrix__domain }}
          {% endif %}
          - Internal Service: aibrix-gateway.aibrix-system.svc.cluster.local
          - Namespace: aibrix-system
          
          Monitoring:
          {% if bootstrap_aibrix__monitoring_enabled %}
          - Metrics endpoint: https://{{ bootstrap_aibrix__domain }}/metrics
          {% endif %}
          
          GPU Configuration:
          {% if bootstrap_aibrix__gpu_enabled %}
          - GPU Type: {{ bootstrap_aibrix__gpu_type }}
          - GPU Count per node: {{ bootstrap_aibrix__gpu_count }}
          {% endif %}
          
          Next Steps:
          1. Configure your LLM models
          2. Set up load balancing rules
          3. Configure monitoring dashboards
          4. Test inference endpoints
